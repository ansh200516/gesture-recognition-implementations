import io
import os
import os.path as osp
import pickle
import time
import warnings

import lmdb
import numpy as np
import tensorflow as tf


def get_dist_info():
    """Get distributed information from environment variables.
    This function will parse `TF_CONFIG` to determine the rank and world size.
    If `TF_CONFIG` is not available, it assumes a single-replica setup.

    Returns:
        tuple: A tuple containing rank and world size.
    """
    if 'TF_CONFIG' in os.environ:
        # For TensorFlow distributed training
        import json
        tf_config = json.loads(os.environ['TF_CONFIG'])
        cluster_spec = tf_config.get('cluster', {})
        task_info = tf_config.get('task', {})
        task_type = task_info.get('type')
        task_id = task_info.get('index')

        # 'world_size' is the total number of workers
        world_size = 0
        for Ctype in cluster_spec:
            world_size += len(cluster_spec[Ctype])

        # 'rank' is the global index of the worker
        rank = 0
        for Ctype in cluster_spec:
            if Ctype == task_type:
                rank += task_id
                break
            else:
                rank += len(cluster_spec[Ctype])

        return rank, world_size
    else:
        # Non-distributed setup
        return 0, 1


def _convert_to_tensor(x):
    """Recursively convert numpy arrays to tf.Tensor."""
    if isinstance(x, np.ndarray):
        return tf.convert_to_tensor(x, dtype=tf.float32)
    if isinstance(x, dict):
        return {k: _convert_to_tensor(v) for k, v in x.items()}
    if isinstance(x, (list, tuple)):
        return type(x)(_convert_to_tensor(v) for v in x)
    return x


class LFB(object):
    """Long-Term Feature Bank (LFB) for TensorFlow.

    LFB is proposed in `Long-Term Feature Banks for Detailed Video
    Understanding <https://arxiv.org/abs/1812.05038>`_

    The ROI features of videos are stored in the feature bank. The feature bank
    was generated by inferring with a lfb infer config. This implementation
    assumes that the feature bank files are pickled dictionaries of numpy
    arrays.

    Formally, LFB is a Dict whose keys are video IDs and its values are also
    Dicts whose keys are timestamps in seconds. Example of LFB:

    .. code-block:: Python
        {
            '0f39OWEqJ24': {
                901: tf.constant([[ 1.2760,  1.1965,  ...,  0.0061, -0.0639],
                    [-0.6320,  0.3794,  ..., -1.2768,  0.5684],
                    ...
                ...
            },
            ...
        }

    Args:
        lfb_prefix_path (str): The storage path of lfb.
        max_num_sampled_feat (int): The max number of sampled features.
            Default: 5.
        window_size (int): Window size of sampling long term feature.
            Default: 60.
        lfb_channels (int): Number of the channels of the features stored
            in LFB. Default: 2048.
        dataset_modes (tuple[str] | str): Load LFB of datasets with different
            modes, such as training, validation, testing datasets.
            Default: ('train', 'val').
        device (str): Where to load lfb. Choices are 'GPU', 'CPU' and 'LMDB'.
            Default: 'GPU'.
        lmdb_map_size (int): Map size of lmdb. Default: 4e9.
        construct_lmdb (bool): Whether to construct lmdb. If you have
            constructed lmdb of lfb, you can set to False to skip the
            construction. Default: True.
    """

    def __init__(self,
                 lfb_prefix_path,
                 max_num_sampled_feat=5,
                 window_size=60,
                 lfb_channels=2048,
                 dataset_modes=('train', 'val'),
                 device='GPU',
                 lmdb_map_size=4e9,
                 construct_lmdb=True):
        if not osp.exists(lfb_prefix_path):
            raise ValueError(
                f'lfb prefix path {lfb_prefix_path} does not exist!')
        self.lfb_prefix_path = lfb_prefix_path
        self.max_num_sampled_feat = max_num_sampled_feat
        self.window_size = window_size
        self.lfb_channels = lfb_channels
        if not isinstance(dataset_modes, tuple):
            assert isinstance(dataset_modes, str)
            dataset_modes = (dataset_modes, )
        self.dataset_modes = dataset_modes
        self.device = device.upper()

        self.rank, self.world_size = get_dist_info()

        # Loading LFB
        tf_device = f'/{self.device}:0' if self.device in ['GPU', 'CPU'] else '/CPU:0'
        with tf.device(tf_device):
            if self.device in ['GPU', 'CPU']:
                if self.world_size > 1 and self.device == 'CPU':
                    warnings.warn(
                        'If distributed training is used with multi-GPUs, lfb '
                        'will be loaded multiple times on RAM. In this case, '
                        "'LMDB' is recomended.", UserWarning)
                self.load_lfb()
            elif self.device == 'LMDB':
                self.lmdb_map_size = lmdb_map_size
                self.construct_lmdb = construct_lmdb
                self.lfb_lmdb_path = osp.normpath(
                    osp.join(self.lfb_prefix_path, 'lmdb'))

                if self.rank == 0 and self.construct_lmdb:
                    print('Constructing LFB lmdb...')
                    self.load_lfb_on_lmdb()

                # Synchronizes all processes to make sure lfb lmdb exist.
                if self.world_size > 1:
                    # A simple file-based barrier
                    barrier_file = osp.join(self.lfb_prefix_path, 'lmdb.lock')
                    if self.rank == 0:
                        with open(barrier_file, 'w') as f:
                            f.write('lock')
                    else:
                        while not osp.exists(barrier_file):
                            time.sleep(1)
                self.lmdb_env = lmdb.open(self.lfb_lmdb_path, readonly=True)
            else:
                raise ValueError("Device must be 'GPU', 'CPU' or 'LMDB', ",
                                 f'but get {self.device}.')

    def load_lfb(self):
        self.lfb = {}
        for dataset_mode in self.dataset_modes:
            lfb_path = osp.normpath(
                osp.join(self.lfb_prefix_path, f'lfb_{dataset_mode}.pkl'))
            print(f'Loading LFB from {lfb_path}...')
            try:
                with open(lfb_path, 'rb') as f:
                    lfb_data = pickle.load(f)
            except Exception as e:
                warnings.warn(
                    'Failed to load LFB with pickle. This might be because the '
                    'file was created with `torch.save`. Please convert the '
                    'LFB files to a pickle-compatible format (e.g., dicts of '
                    f'numpy arrays). Original error: {e}')
                raise
            self.lfb.update(_convert_to_tensor(lfb_data))
        print(f'LFB has been loaded on {self.device}.')

    def load_lfb_on_lmdb(self):
        lfb = {}
        for dataset_mode in self.dataset_modes:
            lfb_path = osp.normpath(
                osp.join(self.lfb_prefix_path, f'lfb_{dataset_mode}.pkl'))
            try:
                with open(lfb_path, 'rb') as f:
                    lfb_data = pickle.load(f)
            except Exception as e:
                warnings.warn(
                    'Failed to load LFB with pickle. This might be because the '
                    'file was created with `torch.save`. Please convert the '
                    'LFB files to a pickle-compatible format (e.g., dicts of '
                    f'numpy arrays). Original error: {e}')
                raise
            lfb.update(lfb_data)

        lmdb_env = lmdb.open(self.lfb_lmdb_path, map_size=self.lmdb_map_size)
        for key, value in lfb.items():
            with lmdb_env.begin(write=True) as txn:
                serialized_value = pickle.dumps(value)
                txn.put(key.encode(), serialized_value)

        print(f'LFB lmdb has been constructed on {self.lfb_lmdb_path}!')

    def sample_long_term_features(self, video_id, timestamp):
        if self.device == 'LMDB':
            with self.lmdb_env.begin(write=False) as txn:
                buf = txn.get(video_id.encode())
                video_features_np = pickle.loads(buf)
                video_features = _convert_to_tensor(video_features_np)
        else:
            video_features = self.lfb[video_id]

        # Sample long term features.
        window_size, K = self.window_size, self.max_num_sampled_feat
        start = timestamp - (window_size // 2)

        lt_feats = tf.Variable(tf.zeros((window_size * K, self.lfb_channels), dtype=tf.float32))

        for idx, sec in enumerate(range(start, start + window_size)):
            if sec in video_features:
                feats_at_sec = video_features[sec]
                num_feat = feats_at_sec.shape[0]

                num_feat_int = int(num_feat)
                num_feat_sampled = min(num_feat_int, K)

                if num_feat_sampled > 0:
                    random_lfb_indices = np.random.choice(
                        range(num_feat_int), num_feat_sampled, replace=False)

                    for k, rand_idx in enumerate(random_lfb_indices):
                        lt_feats[idx * K + k].assign(feats_at_sec[rand_idx])

        return tf.convert_to_tensor(lt_feats)


    def __getitem__(self, img_key):
        """Sample long term features like `lfb['0f39OWEqJ24,0902']` where `lfb`
        is a instance of class LFB."""
        video_id, timestamp = img_key.split(',')
        return self.sample_long_term_features(video_id, int(timestamp))

    def __len__(self):
        """The number of videos whose ROI features are stored in LFB."""
        if self.device == 'LMDB':
            with self.lmdb_env.begin(write=False) as txn:
                return txn.stat()['entries']
        return len(self.lfb) 